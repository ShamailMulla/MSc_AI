{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title ## Coursework: Illustrated Prose & Poetry of Comical Hauntings"
      ],
      "metadata": {
        "id": "QE-DOtX22sga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Installations & Imports\n",
        "from IPython.display import clear_output\n",
        "from IPython.core.display import display, HTML\n",
        "import os\n",
        "import ipywidgets\n",
        "\n",
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')\n",
        "drive_path = '/content/drive/My Drive/MSc/Sem2_CC/'\n",
        "os.chdir(drive_path)\n",
        "\n",
        "# to save and print images in the final result\n",
        "jupyter_path = '/usr/local/share/jupyter'\n",
        "temp_img_dir = '/nbextensions/generated_images/'\n",
        "os.makedirs(jupyter_path+temp_img_dir, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  import gpt_2_simple as gpt2\n",
        "except Exception as e:\n",
        "  # if this is running for the first time then download required libraries\n",
        "  print(e)\n",
        "  !sudo apt-get install wkhtmltopdf\n",
        "  !pip install -r requirements.txt\n",
        "  import gpt_2_simple as gpt2\n",
        "\n",
        "  #Â There is a bug in jax, so uninstall this\n",
        "  !pip uninstall -y jax jaxlib\n",
        "  !pip install diffusers==0.11.1\n",
        "  gpt2.download_gpt2(model_name=\"124M\")\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import gensim\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models.doc2vec import Doc2Vec\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from PIL import Image\n",
        "\n",
        "import pdfkit\n",
        "\n",
        "import helper_functions as help_\n",
        "\n",
        "tf.compat.v1.reset_default_graph()\n",
        "tf_session1 = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(tf_session1, checkpoint='model-500')\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "tVuL040lM8oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Generate a story\n",
        "text = gpt2.generate(tf_session1, run_name='run1', seed=57,\n",
        "                     nsamples=1, top_k=20, prefix=\"<|startoftext|>\", include_prefix=False,\n",
        "                     return_as_list=True)[0]\n",
        "\n",
        "cleaned_lines = help_.clean_text(text)\n",
        "\n",
        "with open('stories/%s.txt'%cleaned_lines[0],'w') as f:\n",
        "  f.writelines(cleaned_lines)"
      ],
      "metadata": {
        "id": "PIElyszG2uAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display illustrated content\n",
        "extra = \"Greg Rutkowski\"\n",
        "html_str = \"<html><body><h1> %s </h1>\"%cleaned_lines[0].title()\n",
        "\n",
        "for i in range(1,len(cleaned_lines)):\n",
        "    print(i,'of',len(cleaned_lines)-1)\n",
        "    content = cleaned_lines[i]\n",
        "\n",
        "    # fetch images only if the sentence has at least 10 words\n",
        "    if len(content.split(' ')) < 10:\n",
        "        html_str += \"\"\"\n",
        "                <tr>\n",
        "                    <td width=\"500\"><h4>%s</h4></td>\n",
        "                </tr>\n",
        "                \"\"\"%content\n",
        "    elif content.isupper():\n",
        "        html_str += \"\"\"\n",
        "                <tr>\n",
        "                    <td><h1>%s</h1></td>\n",
        "                </tr>\n",
        "                \"\"\"%content\n",
        "    else:\n",
        "        prompt = f\"{content}. {extra}\"\n",
        "        image = help_.generate_img(prompt)\n",
        "        img_path = \"temp1_img%s.png\"%i\n",
        "        temp_path = jupyter_path+temp_img_dir+img_path\n",
        "        image.resize((300,300)).save(temp_path)\n",
        "        image.resize((300,300)).save(drive_path+'generated_images/'+img_path.replace('temp1',cleaned_lines[0]))\n",
        "\n",
        "        if image:\n",
        "            html_str += \"\"\"\n",
        "            <tr>\n",
        "                <td width=\"500\"><h4>%s</h4></td>\n",
        "                <td><img src=\"%s\" alt=\"img\"/></td>\n",
        "            </tr>\n",
        "            \"\"\"%(content,temp_img_dir+img_path)\n",
        "            display(image)\n",
        "        else:\n",
        "            html_str += \"\"\"\n",
        "            <tr>\n",
        "                <td width=\"500\"><h4>%s</h4></td>\n",
        "            </tr>\n",
        "            \"\"\"%content\n",
        "\n",
        "complete_html = \"\"\"\n",
        "        <table>\n",
        "          %s\n",
        "        </table>\n",
        "    </body></html>\"\"\"%(html_str)\n",
        "\n",
        "with open('/nbextensions/result.html','w') as f:\n",
        "  f.write(complete_html)\n",
        "\n",
        "clear_output()\n",
        "\n",
        "display(HTML(complete_html))"
      ],
      "metadata": {
        "id": "4LsWPqx-WB7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Calculate similarity to training data using cosine distance\n",
        "with open('avg_corpus_embeddings.npy', 'rb') as f:\n",
        "    avg_embeddings = np.load(f)\n",
        "\n",
        "model = Doc2Vec.load('doc2vec_model.model')\n",
        "text_embeddings = model.infer_vector(word_tokenize(text.lower()))\n",
        "cosine_similarity(avg_embeddings.reshape(1, -1), text_embeddings.reshape(1, -1))"
      ],
      "metadata": {
        "id": "Q-TwyU_eRGry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Storing the PDF of the generated story\n",
        "with open('/nbextensions/result.html','w') as f:\n",
        "  f.write(complete_html)\n",
        "\n",
        "pdfkit.from_file('/usr/local/share/jupyter/nbextensions/result.html',\n",
        "                 drive_path+'final_output/%s.pdf'%cleaned_lines[0],\n",
        "                 options={\"enable-local-file-access\": \"\"}, verbose=True)"
      ],
      "metadata": {
        "id": "xBstz1o9q9G_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}